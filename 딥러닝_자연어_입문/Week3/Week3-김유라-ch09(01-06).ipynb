{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvcB3OdcqzCm"
   },
   "source": [
    "# 09. 워드 임베딩(Word Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27Iuseuoq18h"
   },
   "source": [
    "- 각 단어를 인공 신경망 학습을 통해 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQoOrDTdqrv0"
   },
   "source": [
    "## 09-01 워드 임베딩(Word Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNw94qi2qoBA"
   },
   "source": [
    "### 1. 희소 표현(Sparse Representation)\n",
    "- 희소 벡터의 문제점\n",
    "  - 단어의 개수가 늘어나면 벡터의 차원이 한없이 커짐\n",
    "  - 벡터 표현은 공간적 낭비를 불러일으킴\n",
    "  - ex) 원-핫 벡터, DTM\n",
    "\n",
    "### 2. 밀집 표현(Dense Representation)\n",
    "- 밀집 표현은 벡터의 차원을 단어 집합의 크기로 상정X\n",
    "- 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤\n",
    "- 0과 1뿐만 아닌 실수값을 가지게 됨\n",
    "- 강아지의 희소표현\n",
    "      강아지 = [ 0 0 0 0 1 0 0 0 0 0 0 0 ... 중략 ... 0] # 이때 1 뒤의 0의 수는 9995개. 차원은 10,000\n",
    "- 강아지의 벡터표현\n",
    "      강아지 = [0.2 1.8 1.1 -2.1 1.1 2.8 ... 중략 ...] # 이 벡터의 차원은 128\n",
    "- 벡터의 차원이 조밀해졌다고 하여 밀집 벡터(dense vector)\n",
    "\n",
    "### 3. 워드 임베딩(Word Embedding)\n",
    "- 단어를 밀집 벡터(dense vector)의 형태로 표현하는 방법\n",
    "- 이 밀집 벡터를 워드 임베딩 과정을 통해 나온 결과라고 하여 임베딩 벡터(embedding vector)\n",
    "- 워드 임베딩 방법: LSA, Word2Vec, FastText, Glove 등\n",
    "\n",
    ". | 원핫 백터 | 임베딩 벡터\n",
    "--- | --- | ---\n",
    "차원 | 고차원(단어 집합의 크기) | 저차원\n",
    "다른 표현 | 희소 벡터의 일종 | 밀집 벡터의 일종\n",
    "표현 방법 | 수동 | 훈련 데이터로부터 학습함\n",
    "값의 타입 | 1과 0 | 실수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3I-U2E_Fq9x8"
   },
   "source": [
    "## 09-02 워드투벡터(Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKftfc_op2wc"
   },
   "source": [
    "- 원-핫 벡터는 단어 벡터 간 유의미한 유사도를 계산할 수 없다\n",
    "-  워드투벡터(Word2Vec): 단어 벡터 간 유의미한 유사도를 반영할 수 있도록 단어의 의미를 수치화 할 수 있는 방법\n",
    "\n",
    "### 1. 희소 표현(Sparse Representation)\n",
    "- 벡터 또는 행렬의 값이 대부분이 0으로 표현되는 방법\n",
    "- 각 단어 벡터간 유의미한 유사성을 표현할 수 없다\n",
    "- 대안으로 단어의 의미를 다차원 공간에 벡터화하는 방법을 사용 -> 분산 표현(distributed representation)\n",
    "\n",
    "### 2. 분산 표현(Distributed Representation)\n",
    "- 분포 가설(distributional hypothesis)이라는 가정 하에 만들어진 표현 방법\n",
    "  - 분포 가설: '비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다'\n",
    "- 분산 표현은 분포 가설을 이용하여 텍스트를 학습하고, 단어의 의미를 벡터의 여러 차원에 분산하여 표현\n",
    "- 희소 표현이 고차원에 각 차원이 분리된 표현 방법\n",
    "- 분산 표현은 저차원에 단어의 의미를 여러 차원에다가 분산 하여 표현\n",
    "- 단어 벡터 간 유의미한 유사도를 계산 가능\n",
    "- Word2Vec가 대표적인 학습방법\n",
    "\n",
    "### 3. CBOW(Continuous Bag of Words)\n",
    "- 주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하는 방법\n",
    "\n",
    "> 예문 : \"The fat cat sat on the mat\"\n",
    "\n",
    "- ['The', 'fat', 'cat', 'on', 'the', 'mat']으로부터 sat을 예측\n",
    "- 중심 단어(center word): 예측해야하는 단어 sat\n",
    "- 주변 단어(context word): 예측에 사용되는 단어들\n",
    "- 윈도우: 중심 단어를 예측하기 위해서 앞, 뒤로 몇개의 단어를 볼지 결정할때, 그 범위\n",
    "- 윈도우 크기가 n이라고 한다면, 실제 중심 단어를 예측하기 위해 참고하려고 하는 주변 단어의 개수는 2n\n",
    "\n",
    "![img](https://wikidocs.net/images/page/22660/%EB%8B%A8%EC%96%B4.PNG)\n",
    "- 슬라이딩 윈도우(sliding window): 윈도우를 옆으로 움직여서 주변 단어와 중심 단어의 선택을 변경해가며 학습을 위한 데이터 셋을 만드는 방법\n",
    "- Word2Vec에서 입력은 모두 원-핫 벡터가 되어야 함\n",
    "- CBOW의 인공 신경망을 간단히 도식화\n",
    "  ![img](https://wikidocs.net/images/page/22660/word2vec_renew_1.PNG)\n",
    "  - 입력층(Input layer)의 입력으로서 앞, 뒤로 사용자가 정한 윈도우 크기 범위 안에 있는 주변 단어들의 원-핫 벡터가 들어가게 되고, 출력층(Output layer)에서 예측하고자 하는 중간 단어의 원-핫 벡터가 레이블로서 필요\n",
    "  - Word2Vec은 은닉층이 1개인 얕은 신경망(shallow neural network)\n",
    "  - 일반적인 은닉층과는 달리 활성화 함수가 존재하지 않으며 룩업 테이블이라는 연산을 담당하는 층으로 투사층(projection layer)이라고 부르기도 함\n",
    "- CBOW의 인공 신경망을 좀 더 확대\n",
    "  ![img](https://wikidocs.net/images/page/22660/word2vec_renew_2.PNG)\n",
    "  - CBOW에서 투사층의 크기 M은 임베딩하고 난 벡터의 차원이 됨\n",
    "  - 입력층과 투사층 사이의 가중치 W는 V × M 행렬\n",
    "  - 투사층에서 출력층사이의 가중치 W'는 M × V 행렬\n",
    "  - 두 행렬은 동일한 행렬을 전치(transpose)한 것이 아니라, 서로 다른 행렬\n",
    "  - 훈련 전에 이 가중치 행렬 W와 W'는 랜덤 값을 가짐\n",
    "  - CBOW는 주변 단어로 중심 단어를 더 정확히 맞추기 위해 계속해서 이 W와 W'를 학습해가는 구조\n",
    "\n",
    "- 입력으로 들어오는 주변 단어의 원-핫 벡터와 가중치 W행렬의 곱 도식화\n",
    "  ![img](https://wikidocs.net/images/page/22660/word2vec_renew_3.PNG)\n",
    "  - 입력 벡터는 원-핫 벡터\n",
    "  - 입력 벡터와 가중치 W 행렬의 곱은 사실 W행렬의 i번째 행을 그대로 읽어오는 것과(lookup) 동일 = 룩업테이블\n",
    "  - CBOW의 목적은 W와 W'를 잘 훈련시키는 것\n",
    "    - 이유: 여기서 lookup해온 W의 각 행벡터가 Word2Vec 학습 후에는 각 단어의 M차원의 임베딩 벡터로 간주되기 때문\n",
    "\n",
    "- 이렇게 주변 단어의 원-핫 벡터에 대해서 가중치 W가 곱해서 생겨진 결과\n",
    "  ![img](https://wikidocs.net/images/page/22660/word2vec_renew_4.PNG)\n",
    "  - 벡터들은 투사층에서 만나 이 벡터들의 평균인 벡터를 구함\n",
    "  - 만약 윈도우 크기 n=2라면, 입력 벡터의 총 개수는 2n이므로 중간 단어를 예측하기 위해서는 총 4개가 입력 벡터로 사용\n",
    "  - 투사층에서 벡터의 평균을 구하는 부분은 CBOW가 Skip-Gram과 다른 차이점\n",
    "\n",
    "- 이렇게 구해진 평균 벡터는 두번째 가중치 행렬 W'와 곱해짐\n",
    "![img](https://wikidocs.net/images/page/22660/word2vec_renew_5.PNG)\n",
    "- 곱셈의 결과로는 원-핫 벡터들과 차원이 V로 동일한 벡터가 나옴\n",
    "- CBOW는 소프트맥스(softmax) 함수를 지나면서 벡터의 각 원소들의 값은 0과 1사이의 실수로, 총 합은 1\n",
    "- 중심 단어의 원-핫 벡터를 $y$로 했을 때, 이 두 벡터값의 오차를 줄이기위해 CBOW는 손실 함수(loss function)로 크로스 엔트로피(cross-entropy) 함수를 사용\n",
    "- 크로스 엔트로피 함수에 중심 단어인 원-핫 벡터와 스코어 벡터를 입력값으로 넣고, 이를 식으로 표현\n",
    "$$cost(\\hat{𝑦}, y) = -\\sum_{j=1}^{V}y_{j}\\ log(\\hat{𝑦_{j}})$$\n",
    "- 역전파(Back Propagation)를 수행하면 W와 W'가 학습\n",
    "- M차원의 크기를 갖는 W의 행렬의 행을 각 단어의 임베딩 벡터로 사용하거나 W와 W' 행렬 두 가지 모두를 가지고 임베딩 벡터를 사용\n",
    "\n",
    "### 4. Skip-gram\n",
    "- 중간에 있는 단어들을 입력으로 주변 단어들을 예측하는 방법\n",
    "- 윈도우 크기=2, 데이터 셋 구성\n",
    "  ![img](https://wikidocs.net/images/page/22660/skipgram_dataset.PNG)\n",
    "- 인공신경망 도식화\n",
    "  ![img](https://wikidocs.net/images/page/22660/word2vec_renew_6.PNG)\n",
    "- 투사층에서 벡터들의 평균을 구하는 과정X\n",
    "- 여러 논문에서 성능 비교를 진행했을 때 전반적으로 Skip-gram이 CBOW보다 성능이 좋다\n",
    "\n",
    "### 5. NNLM Vs. Word2Vec\n",
    "![img](https://wikidocs.net/images/page/22660/word2vec_renew_7.PNG)\n",
    "- n을 학습에 사용하는 단어의 수, m을 임베딩 벡터의 차원, h를 은닉층의 크기, V를 단어 집합의 크기\n",
    "- NNLM은 단어 벡터 간 유사도를 구할 수 있도록 워드 임베딩의 개념을 도입\n",
    "- Word2Vec: 워드 임베딩 자체에 집중하여 NNLM의 느린 학습 속도와 정확도를 개선하여 탄생\n",
    "- NNLM은 다음 단어를 예측하는 언어 모델이 목적이므로 다음 단어를 예측\n",
    "- Word2Vec(CBOW)은 워드 임베딩 자체가 목적이므로 다음 단어가 아닌 중심 단어를 예측하게 하여 학습\n",
    "- NNLM이 예측 단어의 이전 단어들만을 참고\n",
    "- Word2Vec은 예측 단어의 전, 후 단어들을 모두 참고\n",
    "- Word2Vec이 NNLM보다 학습 속도에서 강점을 가지는 이유\n",
    "  - 은닉층 제거\n",
    "  - 소프트맥스와 네거티브 샘플링같은 추가적으로 사용되는 기법\n",
    "- NNLM: $(n × m) + (n × m × h) + (h × V)$\n",
    "- Word2Vec: $(n × m) + (m × log(V))$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPnWgkjCqeKw"
   },
   "source": [
    "## 09-03 영어/한국어 Word2Vec 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PzRUlGGqo85Q"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8Z-W8mWpI-5"
   },
   "source": [
    "### 1. 영어 Word2Vec 만들기\n",
    "#### (1) 훈련 데이터 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o7SGJzfrpD47",
    "outputId": "b386bbc3-fa00-4e60-a0a9-763acd0a4eef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x193adb70d30>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNglmvXCpQHC"
   },
   "source": [
    "#### (2) 훈련 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hgvf0xpZpiVJ",
    "outputId": "417efde8-1ff8-492d-f40d-176f5d57d603"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\YOORA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "hgS7rjk0pHRN"
   },
   "outputs": [],
   "source": [
    "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "target_text = etree.parse(targetXML)\n",
    "\n",
    "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "\n",
    "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\n",
    "# 해당 코드는 괄호로 구성된 내용을 제거.\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "\n",
    "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n",
    "sent_text = sent_tokenize(content_text)\n",
    "\n",
    "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "    normalized_text.append(tokens)\n",
    "\n",
    "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "HrzwKNS0pPnQ",
    "outputId": "52df81e1-0dd8-4214-e875-3abbc6b1552d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플의 개수 : 273424\n"
     ]
    }
   ],
   "source": [
    "print('총 샘플의 개수 : {}'.format(len(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "v6pJqIC_qMWy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
      "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
      "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"
     ]
    }
   ],
   "source": [
    "# 샘플 3개만 출력\n",
    "for line in result[:3]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrQ6Zm5SqPUP"
   },
   "source": [
    "#### (3) Word2Vec 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "scVaG8MlqNjS"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "# size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.\n",
    "# window = 컨텍스트 윈도우 크기\n",
    "# min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)\n",
    "# workers = 학습을 위한 프로세스 수\n",
    "# sg = 0은 CBOW, 1은 Skip-gram.\n",
    "model = Word2Vec(sentences=result, vector_size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "sDX23HSNqWlf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8441445827484131), ('guy', 0.8274192214012146), ('lady', 0.7722979784011841), ('gentleman', 0.7410231828689575), ('boy', 0.7357403635978699), ('soldier', 0.7350995540618896), ('girl', 0.7287259697914124), ('rabbi', 0.6910779476165771), ('kid', 0.6831039190292358), ('david', 0.663507342338562)]\n"
     ]
    }
   ],
   "source": [
    "# man과 유사한 단어 출력\n",
    "model_result = model.wv.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-8h-4LmrUb-"
   },
   "source": [
    "#### (4) Word2Vec 모델 저장하고 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "3b9_VIDgrEaQ"
   },
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('eng_w2v') # 모델 저장\n",
    "loaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\") # 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OtHJVUkyrXC3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8441445827484131), ('guy', 0.8274192214012146), ('lady', 0.7722979784011841), ('gentleman', 0.7410231828689575), ('boy', 0.7357403635978699), ('soldier', 0.7350995540618896), ('girl', 0.7287259697914124), ('rabbi', 0.6910779476165771), ('kid', 0.6831039190292358), ('david', 0.663507342338562)]\n"
     ]
    }
   ],
   "source": [
    "# man과 유사한 단어 출력\n",
    "model_result = loaded_model.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWL6An8UrrS1"
   },
   "source": [
    "### 2. 한국어 Word2Vec 만들기(네이버 영화 리뷰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "anhVNcA-rtN4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "B8IkYtmVrujb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratings.txt', <http.client.HTTPMessage at 0x193adb70fd0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "s_GN1yF1rv0q"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_table('ratings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "T0gJhcifr0BU"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5] # 상위 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "PZLWpG_ErycK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data)) # 리뷰 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "MHsCbM7kr3on"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# NULL 값 존재 유무\n",
    "print(train_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "3ELvonBZr5Mm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "tf2UFMDdr7J6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199992\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data)) # 리뷰 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "SW_116J4r70C"
   },
   "outputs": [],
   "source": [
    "# 정규 표현식을 통한 한글 외 문자 제거\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "_PEzMx-ir9Fp"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...      1\n",
       "2   4655635                   폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고      1\n",
       "3   9251303   와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지      1\n",
       "4  10067386                         안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화      1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5] # 상위 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "i8uh0UOSr-Yl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 199992/199992 [08:59<00:00, 371.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# 불용어 정의\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# 형태소 분석기 OKT를 사용한 토큰화 작업 (다소 시간 소요)\n",
    "okt = Okt()\n",
    "\n",
    "tokenized_data = []\n",
    "for sentence in tqdm(train_data['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    tokenized_data.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "08RMWFPEr_2j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 72\n",
      "리뷰의 평균 길이 : 10.716703668146726\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEGCAYAAABPdROvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAct0lEQVR4nO3df7RV5X3n8fdHMEgSfyHoIkByNdLUH4moSMloOiipEk2jrtGIs1JJQktrSTWNSQNNmth2mOLKRC3pSILVgMaojMbI+COGoI61IeBFifxQRiJEb2AEI1HUQgW/88d+TnO4nHvvvux7fmzu57XWXmef79nPPt8D6te9n2c/jyICMzOzfXVAsxMwM7NycyExM7NCXEjMzKwQFxIzMyvEhcTMzAoZ2OwEGm3o0KHR1tbW7DTMzEplxYoVL0fEsFqf9btC0tbWRnt7e7PTMDMrFUm/7Ooz39oyM7NCXEjMzKwQFxIzMyvEhcTMzApxITEzs0JcSMzMrBAXEjMzK8SFxMzMCnEhMTOzQvrdk+2trm3G/TXjG2ef1+BMzMzy8RWJmZkV4kJiZmaF1K2QSDpI0nJJP5e0RtLfpvgQSYslPZdeD69qM1PSeknrJJ1TFT9V0qr02RxJSvFBku5M8WWS2ur1e8zMrLZ6XpHsBM6KiJOAMcAkSeOBGcCSiBgNLEnvkXQ8MBk4AZgE3CBpQDrXXGAaMDptk1J8KrAtIo4FrgOuqePvMTOzGupWSCLzenp7YNoCOB9YkOILgAvS/vnAHRGxMyI2AOuBcZKGA4dExNKICOCWTm0q57oLmFi5WjEzs8aoax+JpAGSVgJbgMURsQw4KiI2A6TXI9PhI4AXq5p3pNiItN85vkebiNgFvAocUSOPaZLaJbVv3bq1j36dmZlBnQtJROyOiDHASLKrixO7ObzWlUR0E++uTec85kXE2IgYO2xYzQW+zMxsHzVk1FZE/AZ4lKxv46V0u4r0uiUd1gGMqmo2EtiU4iNrxPdoI2kgcCjwSj1+g5mZ1VbPUVvDJB2W9gcDHwWeBRYBU9JhU4B70/4iYHIaiXU0Waf68nT7a7uk8an/47JObSrnugh4OPWjmJlZg9TzyfbhwII08uoAYGFE3CdpKbBQ0lTgBeBigIhYI2khsBbYBUyPiN3pXJcD84HBwINpA7gJuFXSerIrkcl1/D1mZlZD3QpJRDwNnFwj/mtgYhdtZgGzasTbgb36VyJiB6kQmZlZc/jJdjMzK8SFxMzMCnEhMTOzQlxIzMysEBcSMzMrxIXEzMwK8QqJddTVaofgFQ/NbP/hKxIzMyvEhcTMzApxITEzs0JcSMzMrBAXEjMzK8SFxMzMCnEhMTOzQlxIzMysEBcSMzMrxIXEzMwKcSExM7NCXEjMzKwQFxIzMyvEhcTMzApxITEzs0JcSMzMrBAXEjMzK8SFxMzMCqlbIZE0StIjkp6RtEbSlSl+taRfSVqZtnOr2syUtF7SOknnVMVPlbQqfTZHklJ8kKQ7U3yZpLZ6/R4zM6utnlcku4CrIuI4YDwwXdLx6bPrImJM2h4ASJ9NBk4AJgE3SBqQjp8LTANGp21Sik8FtkXEscB1wDV1/D1mZlZD3QpJRGyOiCfT/nbgGWBEN03OB+6IiJ0RsQFYD4yTNBw4JCKWRkQAtwAXVLVZkPbvAiZWrlbMzKwxGtJHkm45nQwsS6HPSXpa0s2SDk+xEcCLVc06UmxE2u8c36NNROwCXgWOqPH90yS1S2rfunVr3/woMzMDGlBIJL0buBv4fES8Rnab6v3AGGAz8M3KoTWaRzfx7trsGYiYFxFjI2LssGHDevcDzMysW3UtJJIOJCsit0XEDwAi4qWI2B0RbwM3AuPS4R3AqKrmI4FNKT6yRnyPNpIGAocCr9Tn15iZWS31HLUl4CbgmYi4tio+vOqwC4HVaX8RMDmNxDqarFN9eURsBrZLGp/OeRlwb1WbKWn/IuDh1I9iZmYNMrCO5z4d+CNglaSVKfbXwKWSxpDdgtoI/ClARKyRtBBYSzbia3pE7E7tLgfmA4OBB9MGWaG6VdJ6siuRyXX8PWZmVkPdCklEPE7tPowHumkzC5hVI94OnFgjvgO4uECaZmZWkJ9sNzOzQlxIzMysEBcSMzMrxIXEzMwKcSExM7NCXEjMzKyQHguJpIslHZz2vyrpB5JOqX9qZmZWBnmuSP4mIrZLOgM4h2y23bn1TcvMzMoiTyGpPF1+HjA3Iu4F3lG/lMzMrEzyPNn+K0nfAT4KXCNpEO5baRltM+6vGd84+7wGZ2Jm/VWegvBJ4CFgUkT8BhgCfKmeSZmZWXn0WEgi4k1gC3BGCu0CnqtnUmZmVh55Rm19HfgyMDOFDgS+V8+kzMysPPLc2roQ+ATwBkBEbAIOrmdSZmZWHnkKyb+nxaICQNK76puSmZmVSZ5CsjCN2jpM0p8APyFbItfMzKzn4b8R8T8k/QHwGvAB4GsRsbjumZmZWSnkWiExFQ4XDzMz20uXhUTSdlK/SOePgIiIQ+qWlZmZlUaXhSQiPDLLzMx6lOvWVprt9wyyK5THI+KpumZlZmalkeeBxK+Rzfh7BDAUmC/pq/VOzMzMyiHPFcmlwMkRsQNA0mzgSeC/1TMxMzMrhzzPkWwEDqp6Pwj4RV2yMTOz0slTSHYCayTNl/RdYDXwuqQ5kuZ01UjSKEmPSHpG0hpJV6b4EEmLJT2XXg+vajNT0npJ6ySdUxU/VdKq9NkcSUrxQZLuTPFlktr28c/BzMz2UZ5bW/ekreLRnOfeBVwVEU+mpXpXSFoMfBpYEhGzJc0AZgBflnQ8MBk4AXgP8BNJvxMRu8lWZJwG/Ax4AJgEPAhMBbZFxLGSJgPXAJfkzM/MzPpAnifbF+zLiSNiM7A57W+X9AwwAjgfmJAOW0BWmL6c4ndExE5gg6T1wDhJG4FDImIpgKRbgAvICsn5wNXpXHcB/yRJaW4wMzNrgDyjtj4u6SlJr0h6TdJ2Sa/15kvSLaeTgWXAUanIVIrNkemwEcCLVc06UmxE2u8c36NNROwCXiUbXdb5+6dJapfUvnXr1t6kbmZmPcjTR3I9MAU4IiIOiYiDe/NUu6R3A3cDn4+I7gqQasSim3h3bfYMRMyLiLERMXbYsGE9pWxmZr2Qp5C8CKzel9tFkg4kKyK3RcQPUvglScPT58PJVl+E7EpjVFXzkcCmFB9ZI75HG0kDgUOBV3qbp5mZ7bs8heSvgAfSiKovVLaeGqWRVTcBz0TEtVUfLSK7wiG93lsVn5xGYh0NjAaWp9tf2yWNT+e8rFObyrkuAh52/4iZWWPlGbU1C3id7FmSd/Ti3KcDfwSskrQyxf4amE22xslU4AXgYoCIWCNpIbCWbMTX9DRiC+ByYD4wmKyT/cEUvwm4NXXMv0I26svMzBooTyEZEhFn9/bEEfE4tfswACZ20WYWWeHqHG8HTqwR30EqRGZm1hx5bm39RFKvC4mZmfUPeQrJdOBHkv5tX4f/mpnZ/ivPA4lel8TMzLqUdz2Sw8lGUf3H5I0R8Vi9kjIzs/LosZBI+mPgSrLnN1YC44GlwFl1zczMzEohTx/JlcBpwC8j4kyyqU48z4iZmQH5CsmOqkWtBkXEs8AH6puWmZmVRZ4+kg5JhwE/BBZL2sZvpygxM7N+Ls+orQvT7tWSHiGbz+pHdc3KzMxKI8808u+XNKjyFmgD3lnPpMzMrDzy9JHcDeyWdCzZ3FZHA9+va1ZmZlYaeQrJ22nRqAuB6yPiL4Hh9U3LzMzKIk8heUvSpWTTtd+XYgfWLyUzMyuTPIXkM8CHgVkRsSGtFfK9+qZlZmZlkWfU1lrgiqr3G8jWFDEzM8t1RWJmZtalXJM2Wt9rm3F/s1MwM+sTXV6RSLo1vV7ZuHTMzKxsuru1daqk9wGflXS4pCHVW6MSNDOz1tbdra1vk02Fcgywgj3XX48UNzOzfq7LK5KImBMRxwE3R8QxEXF01eYiYmZmQL7hv5dLOgn4SAo9FhFP1zctMzMrizyTNl4B3AYcmbbbJP1FvRMzM7NyyDP894+B34uINwAkXUO21O636pmYmZmVQ54HEgXsrnq/mz073ms3km6WtEXS6qrY1ZJ+JWll2s6t+mympPWS1kk6pyp+qqRV6bM5kpTigyTdmeLLJLXl+C1mZtbH8hSS7wLLUhG4GvgZ2XTyPZkPTKoRvy4ixqTtAQBJxwOTgRNSmxskDUjHzwWmAaPTVjnnVGBbRBwLXAdckyMnMzPrYz0Wkoi4lmzixleAbcBnIuL6HO0eS23yOB+4IyJ2prm81gPjJA0HDomIpRERwC3ABVVtFqT9u4CJlasVMzNrnFxTpETEk8CTffSdn5N0GdAOXBUR24ARZFc6FR0p9lba7xwnvb6Y8tsl6VXgCODlPsrTzMxyaPSkjXOB9wNjgM3AN1O81pVEdBPvrs1eJE2T1C6pfevWrb1K2MzMutfQQhIRL0XE7oh4G7gRGJc+6gBGVR06EtiU4iNrxPdoI2kgcChd3EqLiHkRMTYixg4bNqyvfo6ZmdFDIZE0QNJP+urLUp9HxYVAZUTXImByGol1NFmn+vKI2AxslzQ+9X9cBtxb1WZK2r8IeDj1o5iZWQN120cSEbslvSnp0Ih4tTcnlnQ7MAEYKqkD+DowQdIYsltQG4E/Td+zRtJCYC2wC5geEZUhx5eTjQAbDDyYNshGjt0qaT3Zlcjk3uRnZmZ9I09n+w5glaTFwBuVYERc0XUTiIhLa4S7HDYcEbOAWTXi7cCJNeI7gIu7y8HMzOovTyG5P21mZmZ7yTNp4wJJg4H3RsS6BuRkZmYlkmfSxj8EVpKtTYKkMZIW1TkvMzMriTzDf68mG6b7G4CIWAkcXbeMzMysVPIUkl01Rmx5mK2ZmQH5OttXS/qvwABJo4ErgJ/WNy0zMyuLPIXkL4CvADuB24GHgL+vZ1K2t7YZHjhnZq0pz6itN4GvpAWtIiK21z8tMzMrizyjtk6TtAp4muzBxJ9LOrX+qZmZWRnkubV1E/DnEfEvAJLOIFvs6kP1TMwaq6tbZxtnn9fgTMysbPKM2tpeKSIAEfE44NtbZmYGdHNFIumUtLtc0nfIOtoDuAR4tP6pmZlZGXR3a+ubnd5/vWrfz5GYmRnQTSGJiDMbmYiZmZVTj53tkg4jW1Cqrfr4nqaRNzOz/iHPqK0HgJ8Bq4C365uOmZmVTZ5CclBEfKHumZiZWSnlGf57q6Q/kTRc0pDKVvfMzMysFPJckfw78A2y+bYqo7UCOKZeSZmZWXnkKSRfAI6NiJfrnYyZmZVPnltba4A3652ImZmVU54rkt3ASkmPkE0lD3j4r5mZZfIUkh+mzczMbC951iNZ0IhEzMysnPI82b6BGnNrRYRHbZmZWa7O9rHAaWn7CDAH+F5PjSTdLGmLpNVVsSGSFkt6Lr0eXvXZTEnrJa2TdE5V/FRJq9JncyQpxQdJujPFl0lqy/2rzcysz/RYSCLi11XbryLieuCsHOeeD0zqFJsBLImI0cCS9B5JxwOTgRNSmxskDUht5gLTgNFpq5xzKrAtIo4FrgOuyZGTmZn1sTxL7Z5StY2V9GfAwT21i4jHgFc6hc8HKn0uC4ALquJ3RMTOiNgArAfGSRoOHBIRSyMigFs6tamc6y5gYuVqxczMGifPqK3qdUl2ARuBT+7j9x0VEZsBImKzpCNTfATZxJAVHSn2VtrvHK+0eTGda5ekV4EjgL0enJQ0jeyqhve+9737mLqZmdWSZ9RWI9YlqXUlEd3Eu2uzdzBiHjAPYOzYsV6Uy8ysD+UZtTUI+C/svR7J3+3D970kaXi6GhkObEnxDmBU1XEjgU0pPrJGvLpNh6SBwKHsfSvNzMzqLM+trXuBV4EVVD3Zvo8WAVOA2en13qr49yVdC7yHrFN9eUTslrRd0nhgGdkCW9/qdK6lwEXAw6kfxYC2GffXjG+cfV6DMzGz/V2eQjIyIjqPvuqRpNuBCcBQSR1ka77PBhZKmgq8AFwMEBFrJC0E1pL1w0yPiN3pVJeTjQAbDDyYNoCbyKa4X092JTK5tzmamVlxeQrJTyV9MCJW9ebEEXFpFx9N7OL4WcCsGvF24MQa8R2kQmRmZs2Tp5CcAXw6PeG+k6yTOyLiQ3XNzMzMSiFPIflY3bMwM7PSyjP895eNSMTMzMopzxWJ9aCrEVJmZv2BC0k/46JnZn0tz+y/ZmZmXXIhMTOzQlxIzMysEBcSMzMrxJ3tveCOajOzvfmKxMzMCnEhMTOzQlxIzMysEBcSMzMrxIXEzMwKcSExM7NCXEjMzKwQFxIzMyvEDyTaPunq4cyNs89rcCZm1my+IjEzs0JcSMzMrBAXEjMzK8SFxMzMCnEhMTOzQppSSCRtlLRK0kpJ7Sk2RNJiSc+l18Orjp8pab2kdZLOqYqfms6zXtIcSWrG7zEz68+aeUVyZkSMiYix6f0MYElEjAaWpPdIOh6YDJwATAJukDQgtZkLTANGp21SA/M3MzNa69bW+cCCtL8AuKAqfkdE7IyIDcB6YJyk4cAhEbE0IgK4paqNmZk1SLMKSQA/lrRC0rQUOyoiNgOk1yNTfATwYlXbjhQbkfY7x/ciaZqkdkntW7du7cOfYWZmzXqy/fSI2CTpSGCxpGe7ObZWv0d0E987GDEPmAcwduzYmseYmdm+aUohiYhN6XWLpHuAccBLkoZHxOZ022pLOrwDGFXVfCSwKcVH1ohbH/I69WbWk4bf2pL0LkkHV/aBs4HVwCJgSjpsCnBv2l8ETJY0SNLRZJ3qy9Ptr+2SxqfRWpdVtTEzswZpxhXJUcA9aaTuQOD7EfEjSU8ACyVNBV4ALgaIiDWSFgJrgV3A9IjYnc51OTAfGAw8mDYzM2ughheSiHgeOKlG/NfAxC7azAJm1Yi3Ayf2dY5mZpZfKw3/NTOzEnIhMTOzQrywlTVEd6O/vBiWWbn5isTMzApxITEzs0JcSMzMrBAXEjMzK8SFxMzMCvGoLWtZXY308igvs9biKxIzMyvEhcTMzApxITEzs0LcR2J9yuuXmPU/LiS233OnvVl9+daWmZkV4kJiZmaF+NaWWSe+FWbWOy4k1nRl76BvZuFx0bNW4EJi1iK8ZouVlQuJlU7Zr2DM9jcuJGb9iG+FWT24kFi/5Ssbs77hQmKWUzMLj4uetTIXErM6ceGx/sKFxMy65D4Vy6P0hUTSJOAfgQHAP0fE7CanZFY6voKxIkpdSCQNAP4n8AdAB/CEpEURsba5mZnt33ylYtVKXUiAccD6iHgeQNIdwPmAC4lZEzTiysbFqvWUvZCMAF6set8B/F7ngyRNA6alt69LWreP3zcUeHkf2zZaWXJ1nn2rLHnCPuaqa+qQSffK8mda7zzf19UHZS8kqhGLvQIR84B5hb9Mao+IsUXP0whlydV59q2y5AnlydV59qzs08h3AKOq3o8ENjUpFzOzfqnsheQJYLSkoyW9A5gMLGpyTmZm/Uqpb21FxC5JnwMeIhv+e3NErKnjVxa+PdZAZcnVefatsuQJ5cnVefZAEXt1KZiZmeVW9ltbZmbWZC4kZmZWiAtJTpImSVonab2kGc3Op0LSzZK2SFpdFRsiabGk59Lr4c3MMeU0StIjkp6RtEbSla2Yq6SDJC2X9POU59+2Yp4VkgZIekrSfel9q+a5UdIqSSsltadYy+Uq6TBJd0l6Nv2z+uEWzfMD6c+ysr0m6fPNytWFJIeqqVg+BhwPXCrp+OZm9R/mA5M6xWYASyJiNLAkvW+2XcBVEXEcMB6Ynv4MWy3XncBZEXESMAaYJGk8rZdnxZXAM1XvWzVPgDMjYkzVsw6tmOs/Aj+KiN8FTiL7s225PCNiXfqzHAOcCrwJ3EOzco0Ibz1swIeBh6rezwRmNjuvqnzagNVV79cBw9P+cGBds3OskfO9ZHOktWyuwDuBJ8lmS2i5PMmem1oCnAXc18p/98BGYGinWEvlChwCbCANQmrVPGvkfTbwr83M1Vck+dSaimVEk3LJ46iI2AyQXo9scj57kNQGnAwsowVzTbeLVgJbgMUR0ZJ5AtcDfwW8XRVrxTwhm3Hix5JWpCmLoPVyPQbYCnw33S78Z0nvovXy7GwycHvab0quLiT55JqKxXom6d3A3cDnI+K1ZudTS0TsjuyWwUhgnKQTm5zSXiR9HNgSESuanUtOp0fEKWS3h6dL+v1mJ1TDQOAUYG5EnAy8QQvcxupOehD7E8D/amYeLiT5lG0qlpckDQdIr1uanA8Akg4kKyK3RcQPUrglcwWIiN8Aj5L1QbVanqcDn5C0EbgDOEvS92i9PAGIiE3pdQvZvfxxtF6uHUBHugIFuIussLRantU+BjwZES+l903J1YUkn7JNxbIImJL2p5D1RzSVJAE3Ac9ExLVVH7VUrpKGSTos7Q8GPgo8S4vlGREzI2JkRLSR/fP4cER8ihbLE0DSuyQdXNknu6e/mhbLNSL+H/CipA+k0ESyJSlaKs9OLuW3t7WgWbk2u6OoLBtwLvB/gV8AX2l2PlV53Q5sBt4i+z+qqcARZJ2wz6XXIS2Q5xlktwOfBlam7dxWyxX4EPBUynM18LUUb6k8O+U8gd92trdcnmR9Dz9P25rKvz8tmusYoD39/f8QOLwV80y5vhP4NXBoVawpuXqKFDMzK8S3tszMrBAXEjMzK8SFxMzMCnEhMTOzQlxIzMysEBcS269Jer0O5xwj6dyq91dL+mKB812cZpp9pG8y3Oc8Nkoa2swcrJxcSMx6bwzZMzB9ZSrw5xFxZh+e06xhXEis35D0JUlPSHq6ap2RtnQ1cGNaf+TH6Yl2JJ2Wjl0q6RuSVqeZDf4OuCStA3FJOv3xkh6V9LykK7r4/kvTmhyrJV2TYl8je1jz25K+0en44ZIeS9+zWtJHUnyupHZVrZeS4hsl/feUb7ukUyQ9JOkXkv4sHTMhnfMeSWslfVvSXv8dkPQpZeuyrJT0nTSR5QBJ81MuqyT9ZcG/EttfNPvpTG/e6rkBr6fXs4F5ZBNwHgDcB/w+2RT8u4Ax6biFwKfS/mrgP6X92aSp+oFPA/9U9R1XAz8FBgFDyZ42PrBTHu8BXgCGkU0O+DBwQfrsUWBsjdyv4rdPgQ8ADk77Q6pijwIfSu83Apen/evIns4+OH3nlhSfAOwge9p8ALAYuKiq/VDgOOB/V34DcANwGdm6F4ur8jus2X+/3lpj8xWJ9Rdnp+0psjVGfhcYnT7bEBEr0/4KoC3Nt3VwRPw0xb/fw/nvj4idEfEy2UR5R3X6/DTg0YjYGhG7gNvICll3ngA+I+lq4IMRsT3FPynpyfRbTiBbbK2iMgfcKmBZRGyPiK3AjsocYsDyiHg+InaTTbFzRqfvnUhWNJ5I0+lPJCs8zwPHSPqWpElAS87ebI03sNkJmDWIgH+IiO/sEczWRtlZFdoNDKb20gHd6XyOzv9u9fZ8RMRjabr184Bb062vfwG+CJwWEdskzQcOqpHH251yersqp87zInV+L2BBRMzsnJOkk4BzgOnAJ4HP9vZ32f7HVyTWXzwEfDath4KkEZK6XPQnIrYB25UtswvZDLsV28luGfXGMuA/SxqqbOnmS4H/010DSe8juyV1I9nMyaeQreL3BvCqpKPIphHvrXFpJusDgEuAxzt9vgS4qPLno2wd8PelEV0HRMTdwN+kfMx8RWL9Q0T8WNJxwNJsRnteBz5FdvXQlanAjZLeIOuLeDXFHwFmpNs+/5Dz+zdLmpnaCnggInqa4nsC8CVJb6V8L4uIDZKeIptF93ngX/N8fydLyfp8Pgg8RrY+SHWuayV9lWxFwwPIZpaeDvwb2eqBlf8B3euKxfonz/5r1gVJ746I19P+DLK1sK9sclqFSJoAfDEiPt7kVGw/4isSs66dl64iBgK/JButZWad+IrEzMwKcWe7mZkV4kJiZmaFuJCYmVkhLiRmZlaIC4mZmRXy/wF/TO/ot0qvowAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 리뷰 길이 분포 확인\n",
    "print('리뷰의 최대 길이 :',max(len(review) for review in tokenized_data))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, tokenized_data))/len(tokenized_data))\n",
    "plt.hist([len(review) for review in tokenized_data], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "pkEXsq6Bsoec"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Word2Vec으로 토큰화 된 네이버 영화 리뷰 데이터 학습\n",
    "model = Word2Vec(sentences = tokenized_data, vector_size = 100, window = 5, min_count = 5, workers = 4, sg = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "X22j3DwrspaZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16477, 100)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 완성된 임베딩 매트릭스의 크기 확인\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Kv4xPNJuszHf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('한석규', 0.8581089377403259), ('송강호', 0.8570764660835266), ('윤제문', 0.8477233052253723), ('서영희', 0.8421652913093567), ('류덕환', 0.8383586406707764), ('김수현', 0.836716890335083), ('안성기', 0.8350959420204163), ('박중훈', 0.8318821787834167), ('이정재', 0.8314324617385864), ('문소리', 0.8308103084564209)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"최민식\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "3VqR4D79s06N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('슬래셔', 0.8643483519554138), ('호러', 0.8476483225822449), ('무협', 0.8456701040267944), ('물의', 0.8381571769714355), ('정통', 0.8339495062828064), ('느와르', 0.8298413753509521), ('무비', 0.8216230273246765), ('블랙', 0.8113743662834167), ('블록버스터', 0.8083066940307617), ('물', 0.7811896800994873)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"히어로\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hwpg5o0xs2_Y"
   },
   "source": [
    "### 3. 사전 훈련된 Word2Vec 임베딩(Pre-trained Word2Vec embedding) 소개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "c8m84-ZVtUTx"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import urllib.request\n",
    "\n",
    "# 구글의 사전 훈련된 Word2Vec 모델을 로드.\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "DwVG8xjPtW-9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000000, 300)\n"
     ]
    }
   ],
   "source": [
    "# 모델 크기 확인\n",
    "print(word2vec_model.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "vnDWOmCBtdht"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40797037\n",
      "0.057204384\n"
     ]
    }
   ],
   "source": [
    "# 두 단어간 유사도 계산\n",
    "print(word2vec_model.similarity('this', 'is'))\n",
    "print(word2vec_model.similarity('post', 'book'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "VlRsysIdtg5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11279297 -0.02612305 -0.04492188  0.06982422  0.140625    0.03039551\n",
      " -0.04370117  0.24511719  0.08740234 -0.05053711  0.23144531 -0.07470703\n",
      "  0.21875     0.03466797 -0.14550781  0.05761719  0.00671387 -0.00701904\n",
      "  0.13183594 -0.25390625  0.14355469 -0.140625   -0.03564453 -0.21289062\n",
      " -0.24804688  0.04980469 -0.09082031  0.14453125  0.05712891 -0.10400391\n",
      " -0.19628906 -0.20507812 -0.27539062  0.03063965  0.20117188  0.17382812\n",
      "  0.09130859 -0.10107422  0.22851562 -0.04077148  0.02709961 -0.00106049\n",
      "  0.02709961  0.34179688 -0.13183594 -0.078125    0.02197266 -0.18847656\n",
      " -0.17480469 -0.05566406 -0.20898438  0.04858398 -0.07617188 -0.15625\n",
      " -0.05419922  0.01672363 -0.02722168 -0.11132812 -0.03588867 -0.18359375\n",
      "  0.28710938  0.01757812  0.02185059 -0.05664062 -0.01251221  0.01708984\n",
      " -0.21777344 -0.06787109  0.04711914 -0.00668335  0.08544922 -0.02209473\n",
      "  0.31835938  0.01794434 -0.02246094 -0.03051758 -0.09570312  0.24414062\n",
      "  0.20507812  0.05419922  0.29101562  0.03637695  0.04956055 -0.06689453\n",
      "  0.09277344 -0.10595703 -0.04370117  0.19726562 -0.03015137  0.05615234\n",
      "  0.08544922 -0.09863281 -0.02392578 -0.08691406 -0.22460938 -0.16894531\n",
      "  0.09521484 -0.0612793  -0.03015137 -0.265625   -0.13378906  0.00139618\n",
      "  0.01794434  0.10107422  0.13964844  0.06445312 -0.09765625 -0.11376953\n",
      " -0.24511719 -0.15722656  0.00457764  0.12988281 -0.03540039 -0.08105469\n",
      "  0.18652344  0.03125    -0.09326172 -0.04760742  0.23730469  0.11083984\n",
      "  0.08691406  0.01916504  0.21386719 -0.0065918  -0.08984375 -0.02502441\n",
      " -0.09863281 -0.05639648 -0.26757812  0.19335938 -0.08886719 -0.25976562\n",
      "  0.05957031 -0.10742188  0.09863281  0.1484375   0.04101562  0.00340271\n",
      " -0.06591797 -0.02941895  0.20019531 -0.00521851  0.02355957 -0.13671875\n",
      " -0.12597656 -0.10791016  0.0067749   0.15917969  0.0145874  -0.15136719\n",
      "  0.07519531 -0.02905273  0.01843262  0.20800781  0.25195312 -0.11523438\n",
      " -0.23535156  0.04101562 -0.11035156  0.02905273  0.22460938 -0.04272461\n",
      "  0.09667969  0.11865234  0.08007812  0.07958984  0.3125     -0.14941406\n",
      " -0.234375    0.06079102  0.06982422 -0.14355469 -0.05834961 -0.36914062\n",
      " -0.10595703  0.00738525  0.24023438 -0.10400391 -0.02124023  0.05712891\n",
      " -0.11621094 -0.16894531 -0.06396484 -0.12060547  0.08105469 -0.13769531\n",
      " -0.08447266  0.12792969 -0.15429688  0.17871094  0.2421875  -0.06884766\n",
      "  0.03320312  0.04394531 -0.04589844  0.03686523 -0.07421875 -0.01635742\n",
      " -0.24121094 -0.08203125 -0.01733398  0.0291748   0.10742188  0.11279297\n",
      "  0.12890625  0.01416016 -0.28710938  0.16503906 -0.25585938  0.2109375\n",
      " -0.19238281  0.22363281  0.04541016  0.00872803  0.11376953  0.375\n",
      "  0.09765625  0.06201172  0.12109375 -0.24316406  0.203125    0.12158203\n",
      "  0.08642578  0.01782227  0.17382812  0.01855469  0.03613281 -0.02124023\n",
      " -0.02905273 -0.04541016  0.1796875   0.06494141 -0.13378906 -0.09228516\n",
      "  0.02172852  0.02099609  0.07226562  0.3046875  -0.27539062 -0.30078125\n",
      "  0.08691406 -0.22949219  0.0546875  -0.34179688 -0.00680542 -0.0291748\n",
      " -0.03222656  0.16210938  0.01141357  0.23339844 -0.0859375  -0.06494141\n",
      "  0.15039062  0.17675781  0.08251953 -0.26757812 -0.11669922  0.01330566\n",
      "  0.01818848  0.10009766 -0.09570312  0.109375   -0.16992188 -0.23046875\n",
      " -0.22070312  0.0625      0.03662109 -0.125       0.05151367 -0.18847656\n",
      "  0.22949219  0.26367188 -0.09814453  0.06176758  0.11669922  0.23046875\n",
      "  0.32617188  0.02038574 -0.03735352 -0.12255859  0.296875   -0.25\n",
      " -0.08544922 -0.03149414  0.38085938  0.02929688 -0.265625    0.42382812\n",
      " -0.1484375   0.14355469 -0.03125     0.00717163 -0.16601562 -0.15820312\n",
      "  0.03637695 -0.16796875 -0.01483154  0.09667969 -0.05761719 -0.00515747]\n"
     ]
    }
   ],
   "source": [
    "# book의 벡터\n",
    "print(word2vec_model['book'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uG9lXwp8s53T"
   },
   "source": [
    "- Word2vec는 최근에 들어서는 자연어 처리를 넘어서 추천 시스템에도 사용되고 있는 모델\n",
    "- 적당하게 데이터를 나열해주면 Word2vec은 위치가 근접한 데이터를 유사도가 높은 벡터를 만들어준다는 점에서 착안"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6gQbXY6tt0k"
   },
   "source": [
    "## 09-04 네거티브 샘플링을 이용한 Word2Vec 구현(Skip-Gram with Negative Sampling, SGNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgYEkENquJFN"
   },
   "source": [
    "### 1. 네거티브 샘플링(Negative Sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZfsUK0guKwk"
   },
   "source": [
    "- 단어 집합의 크기가 수만 이상에 달한다면 Word2Vec은 꽤나 학습하기에 무거운 모델\n",
    "- 만약 현재 집중하고 있는 중심 단어와 주변 단어가 '강아지'와 '고양이', '귀여운'과 같은 단어라면, 사실 이 단어들과 별 연관 관계가 없는 '돈가스'나 '컴퓨터'와 같은 수많은 단어의 임베딩 벡터값까지 업데이트하는 것은 비효율적\n",
    "- 네거티브 샘플링은 Word2Vec이 학습 과정에서 전체 단어 집합이 아니라 일부 단어 집합에만 집중할 수 있도록 하는 방법\n",
    "- 주변 단어들을 긍정(positive), 랜덤으로 샘플링 된 단어들을 부정(negative)으로 레이블링\n",
    "- 다중 클래스 분류 문제를 풀던 Word2Vec보다 훨씬 연산량에서 효율적\n",
    "\n",
    "### 2. 네거티브 샘플링 Skip-Gram(Skip-Gram with Negative Sampling, SGNS)\n",
    "- Skip-gram은 중심 단어로부터 주변 단어를 예측하는 모델\n",
    "- SGNS (Skip-Gram with Negative Sampling): 중심 단어와 주변 단어가 모두 입력이 되고, 이 두 단어가 실제로 윈도우 크기 내에 존재하는 이웃 관계인지 그 확률을 예측\n",
    "  ![img](https://wikidocs.net/images/page/69141/%EA%B7%B8%EB%A6%BC1-2.PNG)\n",
    "- 기존의 Skip-gram 데이터셋을 SGNS의 데이터셋으로 바꾸는 과정\n",
    "  ![img](https://wikidocs.net/images/page/69141/%EA%B7%B8%EB%A6%BC3.PNG)\n",
    "  ![img](https://wikidocs.net/images/page/69141/%EA%B7%B8%EB%A6%BC4.PNG)\n",
    "- 두 테이블 중 하나는 입력 1인 중심 단어의 테이블 룩업을 위한 임베딩 테이블이고, 하나는 입력 2인 주변 단어의 테이블 룩업을 위한 임베딩 테이블\n",
    "  ![img](https://wikidocs.net/images/page/69141/%EA%B7%B8%EB%A6%BC5.PNG)\n",
    "- 각 단어는 각 임베딩 테이블을 테이블 룩업하여 임베딩 벡터로 변환\n",
    "  ![img](https://wikidocs.net/images/page/69141/%EA%B7%B8%EB%A6%BC6.PNG)\n",
    "- 그 후의 연산 도식화\n",
    "  ![img](https://wikidocs.net/images/page/69141/%EA%B7%B8%EB%A6%BC7.PNG)\n",
    "- 중심 단어와 주변 단어의 내적값을 이 모델의 예측값으로 하고, 레이블과의 오차로부터 역전파하여 중심 단어와 주변 단어의 임베딩 벡터값을 업데이트\n",
    "- 학습 후에는 좌측의 임베딩 행렬을 임베딩 벡터로 사용할 수도 있고, 두 행렬을 더한 후 사용하거나 두 행렬을 연결(concatenate)해서 사용\n",
    "\n",
    "### 3. 20뉴스그룹 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "RVTfwPN_v3p1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "O10Xdy4at2Rp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 : 11314\n"
     ]
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "print('총 샘플 수 :',len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "L2cyVJyKtjk8"
   },
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "# 특수 문자 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# 전체 단어에 대한 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Gb6yCEArv8om"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "elkEr5Uiv93S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "news_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "U8Yx4ctGv_Ns"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 : 10995\n"
     ]
    }
   ],
   "source": [
    "news_df.dropna(inplace=True)\n",
    "print('총 샘플 수 :',len(news_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "zCaB-9OwwAln"
   },
   "outputs": [],
   "source": [
    "# 불용어를 제거\n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "tokenized_doc = tokenized_doc.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "XciBulEWwClv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 : 10940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YOORA\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:5071: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n"
     ]
    }
   ],
   "source": [
    "# 단어가 1개 이하인 샘플의 인덱스를 찾아서 저장하고, 해당 샘플들은 제거.\n",
    "drop_train = [index for index, sentence in enumerate(tokenized_doc) if len(sentence) <= 1]\n",
    "tokenized_doc = np.delete(tokenized_doc, drop_train, axis=0)\n",
    "print('총 샘플 수 :',len(tokenized_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "GzV6B4uPwEB1"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_doc)\n",
    "\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {value : key for key, value in word2idx.items()}\n",
    "encoded = tokenizer.texts_to_sequences(tokenized_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "fUsfl-GkwFK1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 59, 603, 207, 3278, 1495, 474, 702, 9470, 13686, 5533, 15227, 702, 442, 702, 70, 1148, 1095, 1036, 20294, 984, 705, 4294, 702, 217, 207, 1979, 15228, 13686, 4865, 4520, 87, 1530, 6, 52, 149, 581, 661, 4406, 4988, 4866, 1920, 755, 10668, 1102, 7837, 442, 957, 10669, 634, 51, 228, 2669, 4989, 178, 66, 222, 4521, 6066, 68, 4295], [1026, 532, 2, 60, 98, 582, 107, 800, 23, 79, 4522, 333, 7838, 864, 421, 3825, 458, 6488, 458, 2700, 4730, 333, 23, 9, 4731, 7262, 186, 310, 146, 170, 642, 1260, 107, 33568, 13, 985, 33569, 33570, 9471, 11491]]\n"
     ]
    }
   ],
   "source": [
    "# 상위 2개 샘플 출력\n",
    "print(encoded[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "VqcwDQCGwIcQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 64277\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2idx) + 1 \n",
    "print('단어 집합의 크기 :', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfeB4kFswMW4"
   },
   "source": [
    "### 4. 네거티브 샘플링을 통한 데이터셋 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "YuX1sQe2wKwJ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "# 네거티브 샘플링\n",
    "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "kBMOh2hFwJsR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(away (178), photographic (11883)) -> 0\n",
      "(soldiers (957), government (51)) -> 1\n",
      "(guilt (4989), makes (228)) -> 1\n",
      "(least (87), atrocities (4406)) -> 1\n",
      "(think (6), whole (217)) -> 1\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 샘플인 skip_grams[0] 내 skipgrams로 형성된 데이터셋 확인\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(5):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "          idx2word[pairs[i][0]], pairs[i][0], \n",
    "          idx2word[pairs[i][1]], pairs[i][1], \n",
    "          labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "oUV5UvxmwRQU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 10\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플 수 :',len(skip_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Ms4xlry-wT5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2220\n",
      "2220\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 뉴스그룹 샘플에 대해서 생긴 pairs와 labels의 개수\n",
    "print(len(pairs))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "EscWvRvCwVK6"
   },
   "outputs": [],
   "source": [
    "# 모든 뉴스그램 샘플에 대해 수행\n",
    "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EwBX5fUwY4u"
   },
   "source": [
    "### 5. Skip-Gram with Negative Sampling(SGNS) 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "ZGbKHSFJwbVo"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input\n",
    "from tensorflow.keras.layers import Dot\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "QJZswMPMwc0L"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "# 중심 단어를 위한 임베딩 테이블\n",
    "w_inputs = Input(shape=(1, ), dtype='int32')\n",
    "word_embedding = Embedding(vocab_size, embedding_dim)(w_inputs)\n",
    "\n",
    "# 주변 단어를 위한 임베딩 테이블\n",
    "c_inputs = Input(shape=(1, ), dtype='int32')\n",
    "context_embedding  = Embedding(vocab_size, embedding_dim)(c_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "WiFjhk1-weFW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 1, 100)       6427700     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 1, 100)       6427700     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1, 1)         0           ['embedding[0][0]',              \n",
      "                                                                  'embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1)            0           ['dot[0][0]']                    \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 1)            0           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,855,400\n",
      "Trainable params: 12,855,400\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "dot_product = Dot(axes=2)([word_embedding, context_embedding]) # 임베딩 테이블 거쳐\n",
    "dot_product = Reshape((1,), input_shape=(1, 1))(dot_product) # 내적 수행\n",
    "output = Activation('sigmoid')(dot_product) # 1 또는 0 예측위해 시그모이드 함수를 활성함수로 거침\n",
    "\n",
    "model = Model(inputs=[w_inputs, c_inputs], outputs=output)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "plot_model(model, to_file='model3.png', show_shapes=True, show_layer_names=True, rankdir='TB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "_Q16_qkswq7l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 Loss : 4628.104959990829\n",
      "Epoch : 2 Loss : 3670.1999233383685\n",
      "Epoch : 3 Loss : 3505.9810983911157\n",
      "Epoch : 4 Loss : 3299.0470939371735\n",
      "Epoch : 5 Loss : 3076.3892320385203\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0\n",
    "    for _, elem in enumerate(skip_grams):\n",
    "        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [first_elem, second_elem]\n",
    "        Y = labels\n",
    "        loss += model.train_on_batch(X,Y)  \n",
    "    print('Epoch :',epoch, 'Loss :',loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaXcX4jmwvY6"
   },
   "source": [
    "### 6. 결과 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "qGZbN4MDwuea"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "\n",
    "word_vector_dim = 100\n",
    "f = open('vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-1, word_vector_dim))\n",
    "vectors = model.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()\n",
    "\n",
    "# 모델 로드\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "Vv4UfRuewyhw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wounded', 0.8347063660621643),\n",
       " ('massacred', 0.8222216367721558),\n",
       " ('massacre', 0.819659411907196),\n",
       " ('fighting', 0.805019199848175),\n",
       " ('fleeing', 0.8028205037117004),\n",
       " ('seized', 0.8001793026924133),\n",
       " ('journalists', 0.797465980052948),\n",
       " ('azerbaijan', 0.7973578572273254),\n",
       " ('bloodshed', 0.793043851852417),\n",
       " ('agdam', 0.7881282567977905)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['soldiers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "k-L1R3hUw0U2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('diseases', 0.6267575621604919),\n",
       " ('disease', 0.5989144444465637),\n",
       " ('distinguish', 0.5674646496772766),\n",
       " ('urgent', 0.5670287609100342),\n",
       " ('kidney', 0.5635044574737549),\n",
       " ('infections', 0.562086284160614),\n",
       " ('cure', 0.5605250597000122),\n",
       " ('treatment', 0.5570876598358154),\n",
       " ('clinic', 0.5531479120254517),\n",
       " ('whereever', 0.552707850933075)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['doctor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "FdB5Tilew125"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('enforcement', 0.646109938621521),\n",
       " ('officers', 0.6386284232139587),\n",
       " ('approval', 0.621131181716919),\n",
       " ('engaged', 0.616640567779541),\n",
       " ('armed', 0.6085821986198425),\n",
       " ('protest', 0.6015961170196533),\n",
       " ('carried', 0.5965606570243835),\n",
       " ('defend', 0.5928326845169067),\n",
       " ('filed', 0.5898779034614563),\n",
       " ('deaths', 0.5898562073707581)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['police'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "3GunnsIAw3Vq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('molesting', 0.6993469595909119),\n",
       " ('inflicted', 0.6978085041046143),\n",
       " ('burned', 0.6807896494865417),\n",
       " ('inflicting', 0.6794626116752625),\n",
       " ('aggravated', 0.6759436130523682),\n",
       " ('ultimate', 0.6695354580879211),\n",
       " ('survival', 0.6629530787467957),\n",
       " ('dying', 0.6624512076377869),\n",
       " ('justification', 0.6577073335647583),\n",
       " ('murders', 0.6518142819404602)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['knife'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "WdVLvVxQw4uG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cylinder', 0.5639626979827881),\n",
       " ('mower', 0.5057182312011719),\n",
       " ('trunk', 0.49492892622947693),\n",
       " ('honda', 0.47876763343811035),\n",
       " ('tires', 0.4778628945350647),\n",
       " ('brakes', 0.47401198744773865),\n",
       " ('sensor', 0.47233036160469055),\n",
       " ('exterior', 0.47230878472328186),\n",
       " ('valve', 0.46208059787750244),\n",
       " ('brake', 0.46173378825187683)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['engine'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09-05 글로브(GloVe)\n",
    "- 글로브(Global Vectors for Word Representation, GloVe)는 카운트 기반과 예측 기반을 모두 사용하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 기존 방법론에 대한 비판\n",
    "- LSA는 카운트 기반으로 코퍼스의 전체적인 통계 정보를 고려하기는 하지만 유추 작업(Analogy task)에는 성능이 떨어짐\n",
    "- Word2Vec는 예측 기반으로 단어 간 유추 작업에는 LSA보다 뛰어나지만, 임베딩 벡터가 윈도우 크기 내에서만 주변 단어를 고려하기 때문에 코퍼스의 전체적인 통계 정보를 반영하지 못함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 윈도우 기반 동시 등장 행렬(Window based Co-occurrence Matrix)\n",
    "- 단어의 동시 등장 행렬: 행과 열을 전체 단어 집합의 단어들로 구성하고, i 단어의 윈도우 크기(Window Size) 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬\n",
    "\n",
    ">I like deep learning\n",
    "I like NLP\n",
    "I enjoy flying\n",
    "\n",
    "카운트|I|like|enjoy|deep|learning|NLP|flying\n",
    "--- | --- | ---|---|---|---|---|---\n",
    "I|0|2|1|0|0|0|0\n",
    "like|2|0|0|1|0|1|0\n",
    "enjoy|1|0|0|0|0|0|1\n",
    "deep|0|1|0|0|1|0|0\n",
    "learning|0|0|0|1|0|0|0\n",
    "NLP|0|1|0|0|0|0|0\n",
    "flying|0|0|1|0|0|0|0\n",
    "\n",
    "- 행렬을 전치(Transpose)해도 동일한 행렬이 된다\n",
    "- 이유는 i 단어의 윈도우 크기 내에서 k 단어가 등장한 빈도는 반대로 k 단어의 윈도우 크기 내에서 i 단어가 등장한 빈도와 동일하기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 동시 등장 확률(Co-occurrence Probability)\n",
    "- 동시 등장 확률 $P(k\\ |\\ i)$는 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고, 특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률\n",
    "- $P(k\\ |\\ i)$에서 i를 중심 단어(Center Word), k를 주변 단어(Context Word)라고 했을 때, 위에서 배운 동시 등장 행렬에서 중심 단어 i의 행의 모든 값을 더한 값을 분모로 하고 i행 k열의 값을 분자로 한 값\n",
    "\n",
    "동시 등장 확률과 크기 관계 비(ratio)|k=solid|k=gas|k=water|k=fasion\n",
    "--|---|---|---|---\n",
    "P(k l ice)|0.00019|0.000066|0.003|0.000017\n",
    "P(k l steam)|0.000022|0.00078|0.0022|0.000018\n",
    "P(k l ice) / P(k l steam)|8.9|0.085|1.36|0.96\n",
    "\n",
    "\n",
    "동시 등장 확률과 크기 관계 비(ratio) | k=solid | k=gas | k=water | k=fasion |\n",
    "------|------|------|------|----\n",
    "P(k l ice) | 큰 값 | 작은 값 | 큰 값 | 작은 값 \n",
    "P(k l steam) | 작은 값 | 큰 값 | 큰 값 | 작은 값 \n",
    "P(k l ice) / P(k l steam) | 큰 값 | 작은 값 | 1에 가까움 | 1에 가까움 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 손실 함수(Loss function)\n",
    "- 손실함수 용어\n",
    "  - $X$: 동시 등장 행렬(Co-occurrence Matrix)\n",
    "  - $X_ij$: 중심 단어 i가 등장했을 때 윈도우 내 주변 단어 j가 등장하는 횟수\n",
    "  - $X_i$: $X_{i}: \\sum_j X_{ij}$ 동시 등장 행렬에서 i행의 값을 모두 더한 값\n",
    "  - $P_{ik}$: $P(k\\ |\\ i)$=$\\frac{X_{ik}}{X_{i}}$: 중심 단어 i가 등장했을 때 윈도우 내 주변 단어 k가 등장할 확률\n",
    "        Ex) P(solid l ice) = 단어 ice가 등장했을 때 단어 solid가 등장할 확률\n",
    " \n",
    "  - $\\frac{P_{ik}}{P_{jk}}$: $P_ij$를 $P_jk$로 나눠준 값\n",
    "  - $w_{i}$ : 중심 단어 i의 임베딩 벡터\n",
    "  - $\\tilde{w_{k}}$ : 주변 단어 k의 임베딩 벡터\n",
    "\n",
    "- GloVe: '임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것'\n",
    "- 식: $dot\\ product(w_{i}\\ \\tilde{w_{k}}) \\approx\\ P(k\\ |\\ i) = P_{ik}$\n",
    "- 더 정확한 버전: $dot\\ product(w_{i}\\ \\tilde{w_{k}}) \\approx\\ log\\ P(k\\ |\\ i) = log\\ P_{ik}$\n",
    "\n",
    "- 벡터 $w_{i}, w_{j}, \\tilde{w_{k}}$를 가지고 어떤 함수 $F$를 수행하면, $P_{ik} / P_{jk}$가 나온다는 초기 식으로부터 전개를 시작합니다.\n",
    "$$F(w_{i},\\ w_{j},\\ \\tilde{w_{k}}) = \\frac{P_{ik}}{P_{jk}}$$\n",
    "- 두 단어 사이의 동시 등장 확률의 크기 관계 비(ratio) 정보를 벡터 공간에 인코딩하는 것이 목적 ->  $w_{i}$와 $w_{j}$라는 두 벡터의 차이를 함수 $F$의 입력으로 사용\n",
    "$$F(w_{i} -\\ w_{j},\\ \\tilde{w_{k}}) = \\frac{P_{ik}}{P_{jk}}$$\n",
    "- 우변은 스칼라값이고 좌변은 벡터값 -> 두 입력에 내적(Dot product)을 수행\n",
    "$$F((w_{i} -\\ w_{j})^{T} \\tilde{w_{k}}) = \\frac{P_{ik}}{P_{jk}}$$\n",
    "- $a$와 $b$에 대해서 함수 $F$가 $F(a + b)$과 $F(a)F(b)$와 같도록 만족시켜야 한다\n",
    "$$F((w_{i} -\\ w_{j})^{T} \\tilde{w_{k}}) = \\frac{F(w_{i}^{T}\\tilde{w_{k}})}{F(w_{j}^{T}\\tilde{w_{k}})}$$\n",
    "- 우변은 본래 $\\frac{P_{ik}}{P_{jk}}$\n",
    "$$\\frac{P_{ik}}{P_{jk}} = \\frac{F(w_{i}^{T}\\tilde{w_{k}})}{F(w_{j}^{T}\\tilde{w_{k}})}$$\n",
    "$$F(w_{i}^{T}\\tilde{w_{k}}) = P_{ik} = \\frac{X_{ik}}{X_{i}}$$\n",
    "- 좌변 풀어쓰면 준동형식 형태와 일치\n",
    "$$F(w_{i}^{T}\\tilde{w_{k}}\\ -\\ w_{j}^{T}\\tilde{w_{k}}) = \\frac{F(w_{i}^{T}\\tilde{w_{k}})}{F(w_{j}^{T}\\tilde{w_{k}})}$$\n",
    "- $F$를 지수함수 $exp$라고 가정\n",
    "$$exp(w_{i}^{T}\\tilde{w_{k}}\\ -\\ w_{j}^{T}\\tilde{w_{k}}) = \\frac{exp(w_{i}^{T}\\tilde{w_{k}})}{exp(w_{j}^{T}\\tilde{w_{k}})}$$\n",
    "$$exp(w_{i}^{T}\\tilde{w_{k}}) = P_{ik} = \\frac{X_{ik}}{X_{i}}$$\n",
    "$$w_{i}^{T}\\tilde{w_{k}} = log\\ P_{ik} = log\\ (\\frac{X_{ik}}{X_{i}}) = log\\ X_{ik} - log\\ X_{i}$$\n",
    "- 사실 $w_{i}$와 $\\tilde{w_{k}}$는 두 값의 위치를 서로 바꾸어도 식이 성립해야함 그런데 $log X_i$ 때문에 불가능. 그래서 $log X_i$를 $w_{i}$에 대한 편향 $b_i$로 대체, 같은 이유로 $\\tilde{w_{k}}$에 대한 편향 $\\tilde{b_{k}}$ 추가\n",
    "$$w_{i}^{T}\\tilde{w_{k}} + b_{i} + \\tilde{b_{k}} = log\\ X_{ik}$$\n",
    "- 일반화\n",
    "$$Loss\\ function = \\sum_{m, n=1}^{V}\\ (w_{m}^{T}\\tilde{w_{n}} + b_{m} + \\tilde{b_{n}} - logX_{mn})^{2}$$\n",
    "- 문제점: $X$는 희소 행렬일 가능성이 많아 많은 값이 0이거나 작은 수치를 가지는 경우가 많아서 정보에 도움이 거의 안되는것\n",
    "- 가중치 함수 $f(X_{ik})$ 도입 결정\n",
    "- GloVe에 도입되는 $f(X_{ik})$의 그래프\n",
    "    ![img](https://wikidocs.net/images/page/22885/%EA%B0%80%EC%A4%91%EC%B9%98.PNG)\n",
    "    $$f(x) = min(1,\\ (x/x_{max})^{3/4})$$\n",
    "#### 최종 결과\n",
    "$$Loss\\ function = \\sum_{m, n=1}^{V}\\ f(X_{mn})(w_{m}^{T}\\tilde{w_{n}} + b_{m} + \\tilde{b_{n}} - logX_{mn})^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. GloVe 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 20 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n"
     ]
    }
   ],
   "source": [
    "from glove import Corpus, Glove\n",
    "\n",
    "corpus = Corpus() \n",
    "\n",
    "# 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성\n",
    "corpus.fit(result, window=5)\n",
    "glove = Glove(no_components=100, learning_rate=0.05)\n",
    "\n",
    "# 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20.\n",
    "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.965642635704228), ('guy', 0.8818846786899934), ('young', 0.8492653940396951), ('girl', 0.8448309518871022)]\n"
     ]
    }
   ],
   "source": [
    "print(glove.most_similar(\"man\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('girl', 0.9382888790998231), ('kid', 0.8444211467408833), ('woman', 0.8319182370838567), ('man', 0.8213725355993369)]\n"
     ]
    }
   ],
   "source": [
    "print(glove.most_similar(\"boy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('harvard', 0.8789251656902284), ('mit', 0.8441173702232837), ('stanford', 0.8397840652897561), ('cambridge', 0.834619116606352)]\n"
     ]
    }
   ],
   "source": [
    "print(glove.most_similar(\"university\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fresh', 0.8374100150581377), ('clean', 0.8373212141854532), ('air', 0.8354257431936553), ('electricity', 0.8212560260502819)]\n"
     ]
    }
   ],
   "source": [
    "print(glove.most_similar(\"water\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09-06 패스트텍스트(FastText)\n",
    "- 메커니즘 자체는 Word2Vec의 확장\n",
    "- Word2Vec는 단어를 쪼개질 수 없는 단위로 생각한다면, FastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주\n",
    "- 서브워드(subword)를 고려하여 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 내부 단어(subword)의 학습\n",
    "- FastText에서는 각 단어는 글자 단위 n-gram의 구성으로 취급\n",
    "```python\n",
    "# apple일때, n = 3인 경우\n",
    "<ap, app, ppl, ple, le> \n",
    "# 특별 토큰\n",
    "<apple>\n",
    "# 최종 6개의토큰을 백터화\n",
    "<ap, app, ppl, ple, le>, <apple>\n",
    "```\n",
    "\n",
    "\n",
    "- 실제로 n의 범위 설정 가능, 기본값으로 최소값은 3 최대값은 6임\n",
    "\n",
    "\n",
    "```python\n",
    "# n = 3 ~ 6인 경우\n",
    "<ap, app, ppl, ppl, le>, <app, appl, pple, ple>, <appl, pple>, ..., <apple>\n",
    "\n",
    "```\n",
    "\n",
    "- apple의 벡터값 = 저 위 벡터값들의 총 합\n",
    "```python\n",
    "apple = <ap + app + ppl + ppl + le> + <app + appl + pple + ple> + <appl + pple> + , ..., +<apple>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 모르는 단어(Out Of Vocabulary, OOV)에 대한 대응\n",
    "- FastText의 인공 신경망을 학습한 후에는 데이터 셋의 모든 단어의 각 n-gram에 대해서 워드 임베딩이 됨\n",
    "- 데이터 셋만 충분한다면 위와 같은 내부 단어(Subword)를 통해 모르는 단어(Out Of Vocabulary, OOV)에 대해서도 다른 단어와의 유사도를 계산할 수 있다\n",
    "- 모르는 단어에 제대로 대처할 수 없는 Word2Vec, GloVe와는 다른 점입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 단어 집합 내 빈도 수가 적었던 단어(Rare Word)에 대한 대응\n",
    "- Word2Vec의 경우에는 등장 빈도 수가 적은 단어(rare word)에 대해서는 임베딩의 정확도가 높지 않다는 단점\n",
    "- FastText의 경우, 만약 단어가 희귀 단어라도, 그 단어의 n-gram이 다른 단어의 n-gram과 겹치는 경우라면, Word2Vec과 비교하여 비교적 높은 임베딩 벡터값을 얻음\n",
    "- FastText가 노이즈가 많은 코퍼스에서 강점을 가진 것 또한 이와 같은 이유\n",
    "- 오타가 섞인 단어는 당연히 등장 빈도수가 매우 적으므로 일종의 희귀 단어가 됨\n",
    "-  Word2Vec에서는 오타가 섞인 단어는 임베딩이 제대로 되지 않지만 FastText는 이에 대해서도 일정 수준의 성능을 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 실습으로 비교하는 Word2Vec Vs. FastText\n",
    "\n",
    "#### (1) Word2Vec\n",
    "\n",
    "- Word2Vec에 <code>model.wv.most_similar(\"electrofishing\")</code>를 실행시키면 에러가 남\n",
    "- 학습데이터에 존재하지 않는 단어이기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "model = FastText(result, vector_size=100, window=5, min_count=5, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('electrolux', 0.8675945401191711),\n",
       " ('electrolyte', 0.8651359677314758),\n",
       " ('electroshock', 0.8496968150138855),\n",
       " ('electro', 0.848720133304596),\n",
       " ('airbus', 0.844984233379364),\n",
       " ('electroencephalogram', 0.8342425227165222),\n",
       " ('airbag', 0.8327880501747131),\n",
       " ('electrochemical', 0.826180100440979),\n",
       " ('electrogram', 0.819739043712616),\n",
       " ('electron', 0.8172501921653748)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"electrofishing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 한국어에서의 FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 음절 단위\n",
    "- 예를 들어서 음절 단위의 임베딩의 경우에 n=3일때 ‘자연어처리’라는 단어에 대해 n-gram을 만들어보면 다음과 같음\n",
    "\n",
    "```\n",
    "<자연, 자연어, 연어처, 어처리, 처리>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 자모 단위\n",
    "- 자모 단위(초성, 중성, 종성 단위)로 임베딩하는 시도\n",
    "- 자모 단위로 가게 되면 오타나 노이즈 측면에서 더 강한 임베딩을 기대\n",
    "- 자연어 처리 분리 결과\n",
    "    ```\n",
    "    분리된 결과 : ㅈ ㅏ _ ㅇ ㅕ ㄴ ㅇ ㅓ _ ㅊ ㅓ _ ㄹ ㅣ _\n",
    "\n",
    "    ```\n",
    "- 분리된 결과에 n=3, n-gram 적용\n",
    "    ```\n",
    "    < ㅈ ㅏ, ㅈ ㅏ _, ㅏ _ ㅇ, ... 중략>\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
